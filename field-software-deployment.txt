--------------------------------------------------------------------------
Possible SEA conference talks:

If I were to combine these topics into one talk, what would it be called?

? Field Software Deployment Debt and Data-Ops with Python

== python data file handling framework

highlight the usefulness of tracking all the data files and encoding the
derivations and dependencies of all the data products in one framework






Preface:

Nothing in this talk has been approved by EOL.  I am not speaking for any
other EOL software engineers, I am speaking merely from my own experience
and perspective.

I. stories from the field

Before Jon Corbet became a Linux kernel developer and editor of
LWN, he developed Zebra...

 - Visual integration of a variety of kinds of instrument data, and
   configuration for a set of typical visualizations, for both real-time
   and post-project, because every field project was different.

 - excitement of field developments, meeting scientists' needs in
   real-time, instant gratification

 - immediate problems and the challenge of extra time pressure

Other field stories:

C130 SABL, building and deploying vxworks while flying over the Great Lakes.

P3 circling OKC at low altitude, feeling sick and trying to test and debug
software.

ISS talking to a modem across a lake in a ranger office via a wireless
serial link.

VORTEX driving through the plains

CLIMODE: sitting at a computer and my chair sliding backwards away from the
computer when the ship rolled.



B. pressures in the field

pressure to make the most of an expensive deployment: you send equipment,
instruments, and people half way around the world for 1-2 months ("some
limited window, and within that window to study some limited number of
occurrences of whatever phenomenon you're there to observe") to observe a
specific phenomenon -- you do everything you can to get it riht, and by
definition it's a very customized configuration and integration --
configuration and integration happens where else? in the software.

technical debt is also incurred for data: record as much as possible now,
add measurements in the field with as many instruments as you can field and
fund, with the intention of later on, after the project, dealing with the
format changes and extra research and processing that will be required to
reconcile and finally publish those data.  (which brings about a question:
what if the framework for all the custom processing had to be established
ahead of time, so that we had to prove we could reconcile the observational
data and get the quality control we advertise, so that during the project we
could generate best-quality data in real-time, excepting of course for the
calibrations and sanity checks and fixes for unforeseeable problems which
must still happen post-project?  And by best-quality, I mean if all efforts
for a project ceased, then we could publish immediately the processed data,
since it's the best we knew how to get at that time.)

I suspect modelers have similar pressures -- when it takes days to run an
experiment to study model behavior or produce results for a particular
scenario -- maybe there's a tendency to push all the latest changes in
rather than wait for the next run?  A modeler would have to answer that, I'm
just pointing out that the pressures might be similar to a field project,
any endeavor where we want to capitalize on a limited window with expensive
resources.




II. system administration in the field

 - applies to system adminstration as well as software: making that system
   tweak to support new services and capabilities in the field: firewall
   and routing configurations, configurations for printers and other
   hardware accessories, third-party software installations, desktop
   configurations, user accounts.  Example scenario: scientist or operator
   comes to you in the middle of a field project and says we want to ftp
   the data as they are recorded to this other scientist in real time.  The
   software developer / problem solver in us kicks into gear: I can write a
   script for that, install it in cron, hard-code the network addresses and
   directories, and solve the problem.

II. myths

 - there is not enough time for normal software development practices: no
   time to write tests, or track changes, or write documentation, or
   log issues in an issue tracker

 - I call these myths deliberately.  They are a form of technical debt,
   deploy the expedient fix now and put off all the checks until later.
   It means adopting a feature now and assuming it can be supported
   for all future field deployments.

 - The features are not just software features: it includes all the system
   features accumulated from all the field projects up to now.

 - Users (and developers also) expect to use those features again.  They
   have a long memory.  Also, it is very likely a different user will come
   up with the same need but with a slightly different deployment
   configuration.

III. Technical debt in the field is real

 - EOL has forever had debates and discussions about how well and even
   whether we can say no to certain requests.  It is hard to define all the
   software and system requirements (the "softer" side of field
   deployments) well in advance.  Further, it is often the soft side which
   must accommodate problems in the hard side: changes or additions to
   instruments, equipment failures or re-purposing.

 - The upshot is that we will owe over the long run for every feature we
   have ever offerred for a field deployment, because we won't ever start
   suddenly saying no where we've said yes every time before.  And really
   and truly we don't want to do that either, at least I don't.  I don't
   get excited about software development by telling my users no, I can't
   make that work for you.

 - So the question is how to recognize when we're taking on debt?  Can we
   quantify and minimize the risk, can we compromise and include good
   software development practices in the field?  Maybe the reason we take
   shortcuts in the field is not really time, maybe it's inconvenient
   tools, poor Internet access, culture.

In EOL we talk about putting out fires instead of cultivating the forest,
and probably that happens sometimes for all of us.  So think about the last
fire you had to put out: was that fire a result of a decision made much
earlier for the sake of expedience at the time?  We (software engineers)
tend not to make that connection I think.  We see each problem as a new
problem mneeding a new solution -- and maybe we should look back and check
if what we are really doing is paying interest on a debt from an old
solution.

Freedom requires discipline.  Lots of tools and environments out there --
field work is very distributed and spread out.  Sometimes you find yourself
in the passenger seat of a truck, driving all day from Kansas to Texas, and
debugging python code for sending sounding data in real-time over cellular
Internet connections.  It can be hard to follow a strict software
development process while logged into an embedded Linux computer floating
more than 60,000 feet over the antarctic.  Process may even get in the way.
But that means we need discipline to be aware of the techincal debt we're
taking on.  And we need to be prudent about it: either pay it down quickly
before the interest accumulates, or accept that we'll have to pay more
later.  Maybe we just made a change to some software in the field, and it
seems to fix a problem, so we're happy and we move on the next problem.
Weeks later, perhaps in the same field project or on a different one, are we
going to remember whether we still need that change, why we made it, what it
was supposed to fix?  Maybe it's the sort of problem that we should document
in an issue tracker up front, rather than take on the debt of potentially
having to solve the problem again.


When EOL takes inventory of its software, nominally to identify an plan for
the software which is aging or outdated and needs maintenance before support
becomes critical, it's a little like assessing our technical debt.  From a
certain perspective, the reason we have software at risk of becoming
un-deployable (in other words, software which we as developers deplore
deploying) is because somewhere along the way we decided to put off
maintaining something.  We made a choice -- maybe it was "forced" by field
deployments and lack of time, but we still need to recognize that we took on
debt intending to pay it off later, perhaps without quite appreciating how
much interest there would be.

Important to distinguish debt from risk -- choosing an implementation
language or 3rd party library naturally incurs risk, but not necessarily
debt.  Choosing to keep implementing in Python 2 when you know someday you
will need to port to Python 3 -- that's debt.  Debt is not necessarily good
or bad, but we should recognize the debt that we're intending to pay back,
sometimes unconsciously I think.

debt examples:

choosing not to write tests or to automate tests, in other words intending
to spend time running and evaluating tests manually or to fix whatever bugs
come up from not testing, especially if they turn up during a field project

choosing not to automate system deployment

How would I assess EOL's technical debt?  especially for field deployments?

choosing not to automate backups and backup recovery, unless you have a
policy of not recovering from system failures in the field ==> this is a
situation where we must weigh risk versus debt.  Maybe we think the risk of
a system failure is acceptable.  But what we absolutely shouldn't do is
assume that we can recover from a failure in a day's time by rebuilding a
system manually.  By not developing and testing the backup and recovery
process, we are accepting the debt of debugging that process in the field in
a high-pressure situation.



IV. Principles for software and system deployment in the field

 - these borrow from agile software development practices and from devops
   ideas.

...

A. applying devops to the field

B. applying devops to data: data-ops

"data ops" -- automating data processing and consolidating configuration as
much as possible -- whatever transformations must happen to the data,
whatever software must run and with whatever parameters, should all be
automated.  Like a Makefile for data: identify the dependencies, the
transformations (like Makefile rules), the output formats and metadata (the
targets).  Then "run the build" and "run the tests" specified in the data
Makefile.  If the processing chain, software, configuration, or inputs
change in any of the dependencies, then the affected targets get
regenerated.  Unlike for software builds, data builds should include
documentation in the metadata for how the results are generated, when, and
reasons for changes to the processing.  All of that can generated more
easily if the "build configuration" is consolidated into a "data ops" build
system.

So back to the pie-in-the-sky scenario of producing best-quality data in
real-time during a field project: what if the framework I was talking about
was just such a data build system.  At the beginning of the project we setup
all the known calibrations, corrections, and processing rules, then we just
keep running the build engine through the entire project.  As instruments go
out, or metadata change, or if calibrations change, those get modified in
the build configuration and all the dependencies get updated.  If something
changes which affects the output since the beginning of the project, then
all of that output up to now gets regenerated.  For the duration of the
project, scientists, developers, PIs, technicians can be looking at what the
published data would like if it were released right then.  Maybe some of the
build artifacts are quality control flags to help identify periods which
need to be analyzed manually.

Manual, careful, and subjective analysis will always be required, but
whatever edits are justified must be encoded in the automated processing
chain so they can be repeated automatically and objectively and so the exact
edits can be documented.  For example, radar editing can be rather
subjective and manually intensive, but the edits should be recorded as a
difference to the raw data which can be applied in an automated processing
chain.  Likewise for blank-out periods.  Instead of manually creating a
dataset where periods of bad data are replaced with missing values, the
blank-out period itself should be part of the build configuration which can
be applied to the data automatically, whenever there's a change to the
dependencies.

If any of the calculations of correction factors are objective, then they
can be automated.  They can recomputed continuously throughout the project,
against the data recorded so far, and used to regenerate the data targets
which depend on them.  Once all the instruments are finally turned off and
boxed up and on their way home, we'll have a dataset that is complete except
for any manual analysis that may still be required.

Carrying this Makefile/software build system analogy a little further, it's
like the raw data are source code.  Wouldn't it be cool if we could
distribute the raw data and the "build system configuration" to anyone, so
they could build their own best-quality datasets and tweak the configuration
however they like?  Perhaps the distributed build system includes the source
code for all the required data processing software along with the raw data,
or maybe the build system can be designed to download only the raw data
needed for the targets desired by the user.  This kind of data distribution
gives lots of power and flexibility to the data consumers, but it also
serves as important documentation of how a dataset was generated.  If we
can't give a scientist all the pieces and parameters that were used to
generate a dataset, then I think that impacts the credibility of the data,
at least a little bit.  Sure we can document what we did and why, but how
can anyone verify (including us) that the documentation matches up with a
particular dataset.

Even with the documentation, that doesn't really allow anyone to build the
same dataset we built.  It's like the difference between open source and
proprietary software.  Propietary software can document what it does and how
it works, but without the source you really have no way to confirm what it
does in all situations, much less modify it and improve upon it.  I that's
like most of the data EOL releases.  We're getting better about documenting
what software and version was used for which datasets, and if requested we
could probably re-generate datasets from the original raw data, but it would
take a lot of manual work, and it's certainly not something we could easily
train someone else to do.  Our goal should be the equivalent of open source,
call it open data if you like.  We release the raw data and the exact source
code and build system configuration required to produce the published data
sets, so the sources can be inspected, modified, and even improved and then
the data rebuilt.  Maybe others will even want to submit patches to our data
processing.  Maybe we branch the data processing like we branch software.

I have been trying to do something like this in a python data management
framework written for the EOL Integrated Sounding System.  The ISS is a good
testing ground for this because it contains dozens of disparate raw data
streams which all feed into different derivations and processing software,
and the outputs include artifacts like plots.



V. What has to change:

 - software developers

 - users' expectations have to change too

 - infrastructure: distributed version control, issue tracking in the
   field, software deployment and updates in the field (RPM versus git
   updates and rebuilds/re-installs), software and system testing in the
   field, diagnostics

 - deployment options: continuous integration systems continuously
   generating packages and updating repositories, versus update and install
   on the field system


VI. Conclusion

This idea of technical debt has been a useful perspective to me, though I
admit I may not have a completely accurate understanding of it.  I encourage
you to learn more about it from a more reliable source and add it to your
arsenal of software development practices.

Back to the story of my epic failure during TOGA-COARE.  One way to look at
it was that I took on technical debt by making a software change in the
field.  I knew that sometime down the road I would need to evaluate the
problem more closely and make a more proper fix, but I wanted a fix right
then.  I knew there was some risk, because I remember I actually saved a
copy of the binary in the bin directory, probably with a .bak extension or
an extension of .pre-garys-datastore-change-1993, so obviously I knew I was
taking a chance, and I was expecting someone else to have to discover any
bugs and restore the backup if there was a problem.  And isn't that the
worst kind of debt? the debt we accept but which someone else will have to
pay later?  In TOGA-COARE, I ended up travelling New Zealand with a bike and
a woman from Australia when the debt came due, and my fellow programmer
Michelle had to pay the debt.  When I finally called her a week later
because I had a sinking feeling that maybe something hadn't gone quite
right, she confirmed my fears.  Wherever Michelle is right now, I hope she
forgave me for that.  And for all the other programmers who have had to pay
off debt that I've taken on, especially during field projects, I hope they
can forgive me too.




